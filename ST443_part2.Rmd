---
title: "Estimation of Graphical Models Using Lasso-Related Approaches"
author: "Bowen Deng"
date: "08/11/2019"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r package, include=FALSE}
library(MASS) #mvrnorm
library(glmnet) #lasso
library(glasso)
```

## 1 Generate Samples
***

```{r}
set.seed(20191109)

# number of random variables
p = 50
# number of samples generated
n = 1000
# delta to construct Theta Matrix
delta = 3

Theta <- matrix(0,p,p) + delta*diag(p)
for (i in 2:p){
  for (j in 1:(i-1)){
    Theta[i,j] = 0.5*rbinom(1, 1, 0.1)
    Theta[j,i] = Theta[i,j]
  }
}
Theta = Theta/delta

Sigma = solve(Theta) # Sigma is inverse of Theta

X = mvrnorm(n=n, mu=rep(0, p), Sigma=Sigma)
```

```{r}
# construct set E as a matrix
# column 1: Integer, index i
# column 2: Integer, index j
# column 3: Boolean, 1 for included, 0 for not included 
E_true <- matrix(nrow=0, ncol=3)
for (i in 1:(p-1)){
  for (j in (i+1):p){
    if ( Theta[i,j] ){
      E_true <- rbind(E_true, c(i,j,1))
    }
    else{
      E_true <- rbind(E_true, c(i,j,0))
    }
  }
}
```

## 2 Estimation 
***
### 2.1 Node-wise Lasso Approach
```{r}
# estimate beta
Beta <- matrix(nrow=0, ncol=p)
for (j in 1:p){
  lasso <- glmnet(X[,-j], X[,j], lambda = 0.1)
  beta_j <- coef(lasso)[2:p]
  Beta <- rbind(Beta
                , append(beta_j, NA, after=j-1) # NA just a placeholder, meaningless
                ) 
}

# hence get E_1 and E_2
E_1 <- matrix(nrow=0, ncol=3)
E_2 <- matrix(nrow=0, ncol=3)
for (i in 1:(p-1)){
  for (j in (i+1):p){
    if (Beta[i,j]){
      E_1 <- rbind(E_1, c(i,j,1))
    }
    else{
      E_1 <- rbind(E_1, c(i,j,0))
    }
    if (Beta[i,j] && Beta[j,i]){
      E_2 <- rbind(E_2, c(i,j,1))
    }
    else{
      E_2 <- rbind(E_2, c(i,j,0))
    }
  }
}

# confusion matrix
table(E_1[,3], E_true[,3])
table(E_2[,3], E_true[,3])
```

### 2.2 Graphical Lasso Approach
```{r}
# TODO
glasso <- glasso(cov(X), rho=0.1)

E_3 <- matrix(nrow=0, ncol=3)
for (i in 1:(p-1)){
  for (j in (i+1):p){
    if ( glasso$wi[i,j] ){
      E_3 <- rbind(E_3, c(i,j,1))
    }
    else{
      E_3 <- rbind(E_3, c(i,j,0))
    }
  }
}
table(E_3[,3], E_true[,3])
```

## 3 ROC Curve and Overall Performance 
***

```{r}
len_grid <- 50
grid <- 10 ^ seq(1,-5, length = len_grid)
```

### 3.1 Node-wise Lasso Approach

```{r}
Beta <- array(dim = c(p, p, len_grid))

for (j in 1:p){
  lasso <- glmnet(X[,-j], X[,j], lambda = grid)
  Coef_j <- as.matrix( coef(lasso)[2:p,] ) # drop the 1st element, which is coef of Intercept
  # insert a row as placeholder. e.g. for Xj, the jth row of Coef Matrix should be placeholder
  Beta_j <- matrix(nrow = p, ncol = len_grid)
  if (j==1){
    Beta_j[2:p,] <- Coef_j
  } else if (j==p) {
    Beta_j[1:p-1,] <- Coef_j
  } else {
    Beta_j[1:(j-1),] <- Coef_j[1:(j-1),]
    Beta_j[(j+1):p,] <- Coef_j[j:(p-1),]
  }
  # combine the Beta Matrix
  Beta[j,,] <- Beta_j
}


tpr_1 <- c()
fpr_1 <- c()
tpr_2 <- c()
fpr_2 <- c()

# loop for different lambda
for (k in 1:len_grid){
  
  # for a certain lambda, Beta[,,k] is the Beta Matrix
  # construct set E_1 and E_2 in Matrix form
  E_1 <- matrix(nrow=0, ncol=3)
  E_2 <- matrix(nrow=0, ncol=3)
  for (i in 1:(p-1)){
    for (j in (i+1):p){
      if (Beta[i,j,k]){
        E_1 <- rbind(E_1, c(i,j,1))
      }
      else{
        E_1 <- rbind(E_1, c(i,j,0))
      }
      if (Beta[i,j,k] && Beta[j,i,k]){
        E_2 <- rbind(E_2, c(i,j,1))
      }
      else{
        E_2 <- rbind(E_2, c(i,j,0))
      }
    }
  }
  
  # derive TPR and FPR from the confusion matrix
  Tab_1 <- table(E_1[,3], E_true[,3])
  if (!(0 %in% rownames(Tab_1))){
    Tab_1 <- rbind(c(0,0), Tab_1) # in case all predictions are 1
  }
  if (!(1 %in% rownames(Tab_1))){
    Tab_1 <- rbind(Tab_1, c(0,0)) # in case all predictions are 0
  }
  tpr_1 <- append(tpr_1, Tab_1[2,2]/(sum(Tab_1[,2])) )
  fpr_1 <- append(fpr_1, Tab_1[2,1]/(sum(Tab_1[,1])) ) 
  
  Tab_2 <- table(E_2[,3], E_true[,3])
  if (!(0 %in% rownames(Tab_2))){
    Tab_2 <- rbind(c(0,0), Tab_2)
  }
  if (!(1 %in% rownames(Tab_2))){
    Tab_2 <- rbind(Tab_2, c(0,0))
  }
  tpr_2 <- append(tpr_2, Tab_2[2,2]/(sum(Tab_2[,2])) )
  fpr_2 <- append(fpr_2, Tab_2[2,1]/(sum(Tab_2[,1])) )
}
```

### 3.2 Graphical Lasso Approach
```{r}
tpr_3 <- c()
fpr_3 <- c()

S <- cov(X)

# loop for different lambda
for (k in 1:len_grid){
  glasso <- glasso(S, rho=grid[k])
  
  E_3 <- matrix(nrow=0, ncol=3)
  for (i in 1:(p-1)){
    for (j in (i+1):p){
      if ( glasso$wi[i,j] ){
        E_3 <- rbind(E_3, c(i,j,1))
      }
      else{
        E_3 <- rbind(E_3, c(i,j,0))
      }
    }
  }
  
  # derive TPR and FPR from the confusion matrix
  Tab_3 <- table(E_3[,3], E_true[,3])
  if (!(0 %in% rownames(Tab_3))){
    Tab_3 <- rbind(c(0,0), Tab_3) # in case all predictions are 1
  }
  if (!(1 %in% rownames(Tab_3))){
    Tab_3 <- rbind(Tab_3, c(0,0)) # in case all predictions are 0
  }
  tpr_3 <- append(tpr_3, Tab_3[2,2]/(sum(Tab_3[,2])) )
  fpr_3 <- append(fpr_3, Tab_3[2,1]/(sum(Tab_3[,1])) ) 
}
```

### 3.3 Overall Performance
```{r, fig.height = 3, fig.width = 9, fig.align = "center"}
plot_roc <- function(TPR, FPR, title){
  plot(FPR, TPR, "l"
       , xlab="FPR", ylab="TPR"
       , xlim=c(0,1), ylim=c(0,1)
       , xaxs="i", yaxs="i"
       )
  abline(a=0, b=1, col="red")
  title(main = title)
}

par(mfrow=c(1,3))
plot_roc(tpr_1, fpr_1, "ROC Curve of Method 1")
plot_roc(tpr_2, fpr_2, "ROC Curve of Method 2")
plot_roc(tpr_3, fpr_3, "ROC Curve of Method 3")
```

```{r, results='hold'}
auc <- function(TPR, FPR){
  dFPR <- c(diff(FPR), 0)
  dTPR <- c(diff(TPR), 0)
  return( sum(TPR * dFPR) + sum(dTPR * dFPR)/2 ) 
}

cat("AUC for method 1: ", auc(tpr_1, fpr_1), "\n")
cat("AUC for method 2: ", auc(tpr_2, fpr_2), "\n")
cat("AUC for method 3: ", auc(tpr_3, fpr_3), "\n")
```

## 4 Optimal Tuning Parameters
***
```{r}
neg <- sum(E_true[,3]==0)
pos <- sum(E_true[,3]==1)

fnr_1 <- 1-tpr_1
fnr_2 <- 1-tpr_2
fnr_3 <- 1-tpr_3

error_1 <- (fpr_1*pos+fnr_1*neg)/(pos+neg)
error_2 <- (fpr_2*pos+fnr_2*neg)/(pos+neg)
error_3 <- (fpr_3*pos+fnr_3*neg)/(pos+neg)

plot(log10(grid), error_1, type="l"
     , xlab = "log10(lambda)", ylab = "error rate"
     , col="blue"
     )
lines(log10(grid), error_2, col="red")
lines(log10(grid), error_3, col="green")
legend("topright",legend=c("method 1","method 2", "method 3"), col=c("blue","red", "green"), pch="-")

lambda_1 <- grid[which.min(error_1)]
lambda_2 <- grid[which.min(error_2)]
lambda_3 <- grid[which.min(error_3)]

abline(v=log10(lambda_1), col="blue", lty=2)
abline(v=log10(lambda_2), col="red", lty=4)
abline(v=log10(lambda_3), col="green", lty=2)
```


## 5 Mean and Standard Error of Different Approaches
***
```{r}
set.seed(19970530)
# number of random variables
p <- 50
# number of samples generated
n <- 1000
# delta to construct Theta Matrix
delta <- 3
# length of lambda grid
len_grid <- 50

auc_1 <- c()
auc_2 <- c()
auc_3 <- c()

min_error_1 <- c()
min_error_2 <- c()
min_error_3 <- c()

t = 1
while (t<=50){
  
  # generate Theta, Sigma, X
  Theta <- matrix(0,p,p) + delta*diag(p)
  for (i in 2:p){
    for (j in 1:(i-1)){
      Theta[i,j] = 0.5*rbinom(1, 1, 0.1)
      Theta[j,i] = Theta[i,j]
    }
  }
  Theta = Theta/delta
  Sigma = solve(Theta)
  X = mvrnorm(n=n, mu=rep(0, p), Sigma=Sigma)
  
  # derive E_true
  E_true <- matrix(nrow=0, ncol=3)
  for (i in 1:(p-1)){
    for (j in (i+1):p){
      if ( Theta[i,j] ){
        E_true <- rbind(E_true, c(i,j,1))
      }
      else{
        E_true <- rbind(E_true, c(i,j,0))
      }
    }
  }
  neg <- sum(E_true[,3]==0)
  pos <- sum(E_true[,3]==1)
  if (neg==0 || pos==0){
    next # if samples generated have no neg/pos target, unable to compute TPR/FPR
  }
  
  # Method 1 and 2
  grid <- 10 ^ seq(1,-4, length = len_grid)
  Beta <- array(dim = c(p, p, len_grid))
  for (j in 1:p){
    lasso <- glmnet(X[,-j], X[,j], lambda = grid)
    Coef_j <- as.matrix( coef(lasso)[2:p,] ) # drop the 1st element, which is coef of Intercept
    Beta_j <- matrix(nrow = p, ncol = len_grid)
    if (j==1){
      Beta_j[2:p,] <- Coef_j
    } else if (j==p) {
      Beta_j[1:p-1,] <- Coef_j
    } else {
      Beta_j[1:(j-1),] <- Coef_j[1:(j-1),]
      Beta_j[(j+1):p,] <- Coef_j[j:(p-1),]
    }
    Beta[j,,] <- Beta_j
  }
  tpr_1 <- c()
  fpr_1 <- c()
  tpr_2 <- c()
  fpr_2 <- c()
  for (k in 1:len_grid){
    E_1 <- matrix(nrow=0, ncol=3)
    E_2 <- matrix(nrow=0, ncol=3)
    for (i in 1:(p-1)){
      for (j in (i+1):p){
        if (Beta[i,j,k]){
          E_1 <- rbind(E_1, c(i,j,1))
        }
        else{
          E_1 <- rbind(E_1, c(i,j,0))
        }
        if (Beta[i,j,k] && Beta[j,i,k]){
          E_2 <- rbind(E_2, c(i,j,1))
        }
        else{
          E_2 <- rbind(E_2, c(i,j,0))
        }
      }
    }
    Tab_1 <- table(E_1[,3], E_true[,3])
    if (!(0 %in% rownames(Tab_1))){
      Tab_1 <- rbind(c(0,0), Tab_1) # in case all predictions are 1
    }
    if (!(1 %in% rownames(Tab_1))){
      Tab_1 <- rbind(Tab_1, c(0,0)) # in case all predictions are 0
    }
    tpr_1 <- append(tpr_1, Tab_1[2,2]/(sum(Tab_1[,2])) )
    fpr_1 <- append(fpr_1, Tab_1[2,1]/(sum(Tab_1[,1])) ) 
    Tab_2 <- table(E_2[,3], E_true[,3])
    if (!(0 %in% rownames(Tab_2))){
      Tab_2 <- rbind(c(0,0), Tab_2)
    }
    if (!(1 %in% rownames(Tab_2))){
      Tab_2 <- rbind(Tab_2, c(0,0))
    }
    tpr_2 <- append(tpr_2, Tab_2[2,2]/(sum(Tab_2[,2])) )
    fpr_2 <- append(fpr_2, Tab_2[2,1]/(sum(Tab_2[,1])) )
  }
  
  # Method 3
  tpr_3 <- c()
  fpr_3 <- c()
  S <- cov(X)
  for (k in 1:len_grid){
    glasso <- glasso(S, rho=grid[k])
    E_3 <- matrix(nrow=0, ncol=3)
    for (i in 1:(p-1)){
      for (j in (i+1):p){
        if ( glasso$wi[i,j] ){
          E_3 <- rbind(E_3, c(i,j,1))
        }
        else{
          E_3 <- rbind(E_3, c(i,j,0))
        }
      }
    }
    Tab_3 <- table(E_3[,3], E_true[,3])
    if (!(0 %in% rownames(Tab_3))){
      Tab_3 <- rbind(c(0,0), Tab_3) # in case all predictions are 1
    }
    if (!(1 %in% rownames(Tab_3))){
      Tab_3 <- rbind(Tab_3, c(0,0)) # in case all predictions are 0
    }
    tpr_3 <- append(tpr_3, Tab_3[2,2]/(sum(Tab_3[,2])) )
    fpr_3 <- append(fpr_3, Tab_3[2,1]/(sum(Tab_3[,1])) ) 
  }

  auc_1 <- append(auc_1, auc(tpr_1, fpr_1))
  auc_2 <- append(auc_2, auc(tpr_2, fpr_2))
  auc_3 <- append(auc_3, auc(tpr_3, fpr_3))
  
  fnr_1 <- 1-tpr_1
  fnr_2 <- 1-tpr_2
  fnr_3 <- 1-tpr_3
  error_1 <- (fpr_1*pos+fnr_1*neg)/(pos+neg)
  error_2 <- (fpr_2*pos+fnr_2*neg)/(pos+neg)
  error_3 <- (fpr_3*pos+fnr_3*neg)/(pos+neg)
  min_error_1 <- append(min_error_1, min(error_1))
  min_error_2 <- append(min_error_2, min(error_2))
  min_error_3 <- append(min_error_3, min(error_3))
  
  t <- t+1
}
```

```{r, results="hold"}
cat("AUC of Method 1\n   mean:           ", mean(auc_1), "\n   standard error: ", sd(auc_1), "\n\n")
cat("AUC of Method 2\n   mean:           ", mean(auc_2), "\n   standard error: ", sd(auc_2), "\n\n")
cat("AUC of Method 3\n   mean:           ", mean(auc_3), "\n   standard error: ", sd(auc_3), "\n\n")

cat("Minimum Error Rate of Method 1\n   mean:           ", mean(min_error_1), "\n   standard error: ", sd(min_error_1), "\n\n")
cat("Minimum Error Rate of Method 2\n   mean:           ", mean(min_error_2), "\n   standard error: ", sd(min_error_2), "\n\n")
cat("Minimum Error Rate of Method 2\n   mean:           ", mean(min_error_3), "\n   standard error: ", sd(min_error_3), "\n\n")
```

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}
par(mfrow=c(1,2))
boxplot(auc_1, auc_2, auc_3)
boxplot(min_error_1, min_error_2, min_error_3)
```

