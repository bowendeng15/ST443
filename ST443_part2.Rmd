---
title: "Estimation of Graphical Models Using Lasso-Related Approaches"
author: "Bowen Deng"
date: "08/11/2019"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r package, include=FALSE}
library(MASS) #mvrnorm
library(glmnet) #lasso
library(glasso)

source("functions.R") #import own functions
```

## 1 Generate Samples
***

```{r}
set.seed(666)
# number of random variables
p <- 50
# number of samples generated
n <- 1000
# delta to construct Theta Matrix
delta <- 3

data <- generate(p, n, delta)

data$Theta[1:9,1:9] # part of the Theta Matrix
table(data$E_true[,3]) # corresponding true E set, 3rd column indicate whether the edge is included, table gives summary
```

## 2 Estimation
***
### 2.1 Node-wise Lasso Approach
```{r}
pred.nodewise <- predict.nodewise(data$X, lambda = 0.05)

table(pred.nodewise$E_1[,3], data$E_true[,3]) # compare estimation E_1 and true set E_true
table(pred.nodewise$E_2[,3], data$E_true[,3]) # compare estimation E_2 and true set E_true
```

### 2.2 Graphical Lasso Approach
```{r}
pred.glasso <- predict.glasso(data$X, lambda = 0.05)

table(pred.glasso$E[,3], data$E_true[,3]) # compare estimation E_3 and true set E_true
```

## 3 ROC Curve and Overall Performance
***
To produce a ROC curve, a grid of lambda is needed.
```{r}
len_grid <- 50
grid <- 10 ^ seq(0.5,-2.5, length = len_grid)
```

Run nodewise lasso over the grid. For this data, It took more than 10 secs.
```{r}
start_time <- Sys.time()

perf.nodewise <- performance.nodewise.grid(data$X, data$E_true, grid)

end_time <- Sys.time()
end_time - start_time
```

Run graphical lasso over the grid. For this data, It took less than 1 secs. Glasso is much faster.
```{r}
start_time <- Sys.time()

perf.glasso <- performance.glasso.grid(data$X, data$E_true, grid)

end_time <- Sys.time()
end_time - start_time
```

```{r, fig.height = 3, fig.width = 9, fig.align = "center"}
par(mfrow=c(1,3))
plot.roc(perf.nodewise$tpr_1, perf.nodewise$fpr_1, "ROC Curve of Approach 1")
plot.roc(perf.nodewise$tpr_2, perf.nodewise$fpr_2, "ROC Curve of Approach 2")
plot.roc(perf.glasso$tpr, perf.glasso$fpr, "ROC Curve of Graphical Lasso")
```

```{r, results='hold'}
cat("AUC for method 1: ", perf.nodewise$auc_1, "\n")
cat("AUC for method 2: ", perf.nodewise$auc_2, "\n")
cat("AUC for method 3: ", perf.glasso$auc, "\n")
```

## 4 Optimal Tuning Parameters
***
Plot error rate $(FN+FP)/total$ against lambda, we can find the optimal lambda generate minimum error rate.
```{r}
plot(log10(grid), perf.nodewise$error_1, type="l"
     , xlab = "log10(lambda)", ylab = "error rate"
     , col=2
     )
lines(log10(grid), perf.nodewise$error_2, col=3)
lines(log10(grid), perf.glasso$error, col=4)
legend("topright",legend=c("approach 1","approach 2", "approach 3"), col=c(2,3,4), pch="-")

lambda_1 <- grid[which.min(perf.nodewise$error_1)]
lambda_2 <- grid[which.min(perf.nodewise$error_2)]
lambda_3 <- grid[which.min(perf.glasso$error)]

abline(v=log10(lambda_1), col=2, lty=2)
abline(v=log10(lambda_2), col=3, lty=4)
abline(v=log10(lambda_3), col=4, lty=2)
```

Inspired by K-fold cross validaion, K error rates are generated on each lambda using different folds of data. Then one-standard-deviation rule can be applied.
```{r}
set.seed(666)

len_grid <- 50
grid <- 10 ^ seq(-1.2,-1.6, length = len_grid)

K =10
folds = sample(rep(1:K, length = n))

Error_1 <- c()
Error_2 <- c()
Error_3 <- c()
for (k in 1:K){
  perf.nodewise <- performance.nodewise.grid(data$X[folds!=k,], data$E_true, grid)
  perf.glasso <- performance.glasso.grid(data$X[folds!=k,], data$E_true, grid)

  Error_1 <- rbind(Error_1, perf.nodewise$error_1)
  Error_2 <- rbind(Error_2, perf.nodewise$error_2)
  Error_3 <- rbind(Error_3, perf.glasso$error)
}
```

```{r}
plot.cv.error(Error_1, grid, col=2, ylim=c(0, 0.014))
par(new=T)
plot.cv.error(Error_2, grid, col=3, ylim=c(0, 0.014))
par(new=T)
plot.cv.error(Error_3, grid, col=4, ylim=c(0, 0.014))
legend("topright",legend=c("approach 1","approach 2", "approach 3"), col=c(2,3,4), pch="-")
```

```{r}
par(mfrow=c(2,2))
plot.cv.error(Error_1, grid, zoom = 2)
title(main = "appraoch 1")
plot.cv.error(Error_2, grid, zoom = 2)
title(main = "appraoch 2")
plot.cv.error(Error_3, grid, zoom = 2)
title(main = "appraoch 3")
```




## 5 Mean and Standard Error of Different Approaches
***
```{r}
# it takes about 10 min to run this code chunk
set.seed(666)
p <- 50
n <- 1000
delta <- 3
len_grid <- 50
grid <- 10 ^ seq(0.5,-2.5, length = len_grid)

auc_1 <- c()
auc_2 <- c()
auc_3 <- c()
Error_1 <- c()
Error_2 <- c()
Error_3 <- c()
t = 1
while (t<=50){
  data <- generate(p, n, delta)
  perf.nodewise <- performance.nodewise.grid(data$X, data$E_true, grid)
  perf.glasso <- performance.glasso.grid(data$X, data$E_true, grid)

  auc_1 <- append(auc_1, perf.nodewise$auc_1)
  auc_2 <- append(auc_2, perf.nodewise$auc_2)
  auc_3 <- append(auc_3, perf.glasso$auc)
  Error_1 <- rbind(Error_1, perf.nodewise$error_1)
  Error_2 <- rbind(Error_2, perf.nodewise$error_2)
  Error_3 <- rbind(Error_3, perf.glasso$error)

  t <- t+1
}
```

```{r, results="hold"}
cat("AUC of Method 1\n   mean:           ", mean(auc_1), "\n   standard error: ", sd(auc_1), "\n\n")
cat("AUC of Method 2\n   mean:           ", mean(auc_2), "\n   standard error: ", sd(auc_2), "\n\n")
cat("AUC of Method 3\n   mean:           ", mean(auc_3), "\n   standard error: ", sd(auc_3), "\n\n")

min_error_1 <- apply(Error_1, 1, min)
min_error_2 <- apply(Error_2, 1, min)
min_error_3 <- apply(Error_3, 1, min)
cat("Minimum Error Rate of Method 1\n   mean:           ", mean(min_error_1), "\n   standard error: ", sd(min_error_1), "\n\n")
cat("Minimum Error Rate of Method 2\n   mean:           ", mean(min_error_2), "\n   standard error: ", sd(min_error_2), "\n\n")
cat("Minimum Error Rate of Method 3\n   mean:           ", mean(min_error_3), "\n   standard error: ", sd(min_error_3), "\n\n")
```

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}
par(mfrow=c(1,2))
boxplot(auc_1, auc_2, auc_3)
boxplot(min_error_1, min_error_2, min_error_3)
```

```{r}
plot.cv.error <- function(Error, grid, zoom=NULL, ...){
  mean <- apply(Error, 2, mean)
  sd <- apply(Error, 2, sd)
  id_min <- which.min(mean)
  # set zoom in area
  if ( is.null(zoom) ) id_zoom <- 1:length(grid)
  else id_zoom <- (id_min-zoom):(id_min+zoom)
  # plot
  plot(log10(grid)[id_zoom], mean[id_zoom]
       , "l", xlab="log10(lambda)", ylab="mean error rate"
       ,...)
  abline(h=mean[id_min]+sd[id_min],lty=2)
  abline(v=log10(grid)[id_min],lty=2)
  lines(log10(grid)[id_zoom], (mean+sd)[id_zoom], col="grey", lty=3)
  lines(log10(grid)[id_zoom], (mean-sd)[id_zoom], col="grey", lty=3)
}
```

```{r}
plot.cv.error(Error_1, grid, col=3)
par(new=T)
plot.cv.error(Error_2, grid, col=4)
par(new=T)
plot.cv.error(Error_3, grid, col=5)
```

```{r}
par(mfrow=c(2,2))
plot.cv.error(Error_1, grid, zoom = 28:33)
plot.cv.error(Error_2, grid, zoom = 28:33)
plot.cv.error(Error_3, grid, zoom = 31:36)
```
